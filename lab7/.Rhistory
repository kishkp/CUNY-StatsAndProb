else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
library(XML)
library(RCurl)
library(RJSONIO)
search_term <- "indiana"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
while(more_data==TRUE){
# API URL for NYT Movie reviews
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(getURL(API_url))
# Store data in temporary df
NYT_temp_df <- as.list(json_data[[5]])
# Add rows to the final data frame
NYT_Final_df <- rbind(NYT_Final_df, as.data.frame(NYT_temp_df))
#    if(nrow(NYT_temp_df) < 20) {
if(length(NYT_temp_df[[1]]) < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
View(NYT_Final_df)
View(NYT_Final_df)
library(XML)
library(RCurl)
library(RJSONIO)
search_term <- "star"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
while(more_data==TRUE){
# API URL for NYT Movie reviews
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(getURL(API_url))
# Store data in temporary df
NYT_temp_df <- as.list(json_data[[5]])
# Add rows to the final data frame
NYT_Final_df <- rbind(NYT_Final_df, as.data.frame(NYT_temp_df))
#    if(nrow(NYT_temp_df) < 20) {
if(length(NYT_temp_df[[1]]) < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
library(XML)
library(RCurl)
library(RJSONIO)
search_term <- "star"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
while(more_data==TRUE){
# API URL for NYT Movie reviews
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(getURL(API_url))
# Store data in temporary df
NYT_temp_df <- as.list(json_data[[5]])
# Add rows to the final data frame
NYT_Final_df <- rbind(NYT_Final_df, as.data.frame(NYT_temp_df))
#    if(nrow(NYT_temp_df) < 20) {
if(length(NYT_temp_df[[1]]) < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
library(XML)
library(RCurl)
library(RJSONIO)
search_term <- "indiana"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
while(more_data==TRUE){
# API URL for NYT Movie reviews
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(getURL(API_url))
# Store data in temporary df
NYT_temp_df <- json_data[[5]]
# Add rows to the final data frame
NYT_Final_df <- rbind(NYT_Final_df, NYT_temp_df)
#    if(length(NYT_temp_df[[1]]) < 20) {
if(nrow(NYT_temp_df) < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
json_data[[5]]
unlist(json_data[[5]])
x<-unlist(json_data[[5]])
x<-unlist(json_data[[5]])
library(XML)
library(RCurl)
library(RJSONIO)
search_term <- "indiana"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
library(XML)
library(RCurl)
library(RJSONIO)
search_term <- "indiana"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(getURL(API_url))
# Store data in temporary df
NYT_temp_df <- json_data[[5]]
View(NYT_temp_df)
View(NYT_temp_df)
json_data
class(json_data)
library(stringr)
indy.vec <- unlist(json_data, recursive = TRUE, use.names = TRUE)
indy.vec[str_detect(names(indy.vec), "name")]
indy.vec[str_detect(names(indy.vec), "display")]
length(indy.vec[str_detect(names(indy.vec), "display")])
indy.unlist <- sapply(indy[[1]], unlist)
indy.unlist <- sapply(json_data[[1]], unlist)
indy.unlist <- sapply(json_data[[5]], unlist)
indy.df <- do.call("rbind.fill", lapply(lapply(indy.unlist, t),
data.frame, stringsAsFactors = FALSE))
View(indy.df)
View(indy.df)
names(indy.df)
data3 <- fromJSON(API_url, flatten = TRUE)
data3
data3[[5]]
data3$num_results
data3$results
class(data3$results)
library(XML)
library(RCurl)
library(jsonlite)
search_term <- "star"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
json_data <- fromJSON(getURL(API_url))
library(XML)
library(RCurl)
library(jsonlite)
search_term <- "star"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(API_url, flatten = TRUE)
NYT_Final_df <- rbind(NYT_Final_df, json_data$results)
if(json_data$num_results < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
json_data <- fromJSON(API_url, flatten = TRUE)
NYT_Final_df <- rbind(NYT_Final_df, json_data$results)
if(json_data$num_results < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
if(json_data$num_results < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
library(XML)
library(RCurl)
library(jsonlite)
search_term <- "star"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
while(more_data==TRUE){
# API URL for NYT Movie reviews
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(API_url, flatten = TRUE)
# Store data in temporary df
# NYT_temp_df <- as.list(json_data[[5]])
# Add rows to the final data frame
NYT_Final_df <- rbind(NYT_Final_df, json_data$results)
#    if(nrow(NYT_temp_df) < 20) {
#    if(length(NYT_temp_df[[1]]) < 20) {
if(json_data$num_results < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
library(XML)
library(RCurl)
library(jsonlite)
library(plyr)
search_term <- "vampire"
more_data <- TRUE
next_offset <- 0
NYT_Final_df <- data.frame()
while(more_data==TRUE){
# API URL for NYT Movie reviews
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", params$API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(API_url, flatten = TRUE)
# Store data in temporary df
# NYT_temp_df <- as.list(json_data[[5]])
# Add rows to the final data frame
NYT_Final_df <- rbind.fill(NYT_Final_df, json_data$results)
#    if(nrow(NYT_temp_df) < 20) {
#    if(length(NYT_temp_df[[1]]) < 20) {
if(json_data$num_results < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
library(XML)
library(RCurl)
library(jsonlite)
library(plyr)
search_term <- "vampire"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
while(more_data==TRUE){
# API URL for NYT Movie reviews
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(API_url, flatten = TRUE)
# Store data in temporary df
# NYT_temp_df <- as.list(json_data[[5]])
# Add rows to the final data frame
NYT_Final_df <- rbind.fill(NYT_Final_df, json_data$results)
#    if(nrow(NYT_temp_df) < 20) {
#    if(length(NYT_temp_df[[1]]) < 20) {
if(json_data$num_results < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
View(NYT_Final_df)
View(NYT_Final_df)
library(XML)
library(RCurl)
library(jsonlite)
library(plyr)
search_term <- "star"
more_data <- TRUE
next_offset <- 0
API_key <-"b56a18c8cf8438d9f7310433030d37e4:18:74877465"
NYT_Final_df <- data.frame()
while(more_data==TRUE){
# API URL for NYT Movie reviews
API_url <- paste0("http://api.nytimes.com/svc/movies/v2/reviews/search.json?query=" , search_term, "&api-key=", API_key, "&offset=", next_offset)
# parse json data.
json_data <- fromJSON(API_url, flatten = TRUE)
# Store data in temporary df
# NYT_temp_df <- as.list(json_data[[5]])
# Add rows to the final data frame
NYT_Final_df <- rbind.fill(NYT_Final_df, json_data$results)
#    if(nrow(NYT_temp_df) < 20) {
#    if(length(NYT_temp_df[[1]]) < 20) {
if(json_data$num_results < 20) {
more_data<-FALSE
}
else {
next_offset = next_offset + 20
}
}
head(NYT_Final_df)
summary(NYT_Final_df)
n <- 1259
p <- 0.48
n <- 1259
p <- 0.48
success <- p * n
failure <- (1-p) * n
success > 10
failure > 10
se <- sqrt( (p * (1-p) )/ n)
se
CI <- data.frame(lower=p - me, upper=p + me)
CI
me <- z * (se)
me
se <- sqrt( (p * (1-p) )/ n)
se
z <- qnorm(0.975)
me <- z * (se)
me
CI <- data.frame(lower=p - me, upper=p + me)
CI
CI[upper]
CI["upper"]
p_California <- 0.08
p_Oregon <- 0.088
n_California <- 11545
n_oregon <- 4691
# calculating the difference
p_Diff <- p_Oregon - p_California
# calculating the standard error for the difference.
SE <- sqrt( ((p_California * (1 - p_California)) / n_California) +  ((p_Oregon * (1 - p_Oregon)) / n_Oregon))
# calculating the margin of error for the difference.
me <- qnorm(0.975) * SE
# calculating the 95% confidence interval.
CI <- data.frame(lower=p_Diff - me, upper=p_Diff + me )
CI
CI$lower
wood_proportion <- 426 * 0.048
wood_proportion <- round(426 * 0.048,2)
grassplot_proportion <- round(426 * 0.147,2)
forests_proportion <- round(426 * 0.396,2)
others_proportion <- round(426 * 0.409,2)
wood_proportion <- round(426 * 0.048,2)
grassplot_proportion <- round(426 * 0.147,2)
forests_proportion <- round(426 * 0.396,2)
others_proportion <- round(426 * 0.409,2)
Actual_sites <- c(4, 16, 67, 345)
Proportionate_sites <- c(20.45, 62.62, 168.70, 174.23)
k <- length(Actual_sites)
df <- k - 1
chi_square <- 0
for(i in 1:length(Actual_sites)){
chi_square <- chi_square + ((Actual_sites[i] - Proportionate_sites[i])^2 / Proportionate_sites[i])
}
chi_square
p_value <- pchisq(chi_square, df=df, lower.tail=FALSE)
p_value
2607 / 50739
depression_yes <- 2607 / 50739
depression_no <- 1 - depression_yes
depression_yes <- 2607 / 50739
depression_yes
depression_no <- 1 - depression_yes
depression_no
expected_count <- depression_yes * 6617
k <- 5
df <- k - 1
expected_count <- round(depression_yes * 6617,2)
expected_count
cell_contribution <- (373 - expected_count)^2 / expected_count
cell_contribution
p_value <- pchisq(20.93, df=df, lower.tail=FALSE)
p_value
p_value <- pchisq(20.93, df=df, lower.tail=FALSE)
p_value
n <- 1259
p <- 0.48
# Assuming that the samples are independent
success <- p * n
failure <- (1-p) * n
success > 10
failure > 10
# Success / failure condition is met with at least 10 successes and at least 10 failures
#calculating standard error
se <- sqrt( (p * (1-p) )/ n)
se
z <- qnorm(0.975)
# calculating Margin of Error
me <- z * (se)
me
# Construct the 95% Confidence Interval
CI <- data.frame(lower=p - me, upper=p + me)
CI
p <- 0.48
me <- 0.02
z <- qnorm(0.975)
# calculating standard error for new margin of error
se <- me / z
# working backward for  n in se formula
n <- (p * (1-p)) / se^2
n
p_California <- 0.08
p_Oregon <- 0.088
n_California <- 11545
n_Oregon <- 4691
# calculating the difference
p_Diff <- p_Oregon - p_California
# calculating the standard error for the difference.
SE <- sqrt( ((p_California * (1 - p_California)) / n_California) +  ((p_Oregon * (1 - p_Oregon)) / n_Oregon))
# calculating the margin of error for the difference.
me <- qnorm(0.975) * SE
# calculating the 95% confidence interval.
CI <- data.frame(lower=p_Diff - me, upper=p_Diff + me )
CI
wood_proportion <- round(426 * 0.048,2)
grassplot_proportion <- round(426 * 0.147,2)
forests_proportion <- round(426 * 0.396,2)
others_proportion <- round(426 * 0.409,2)
Actual_sites <- c(4, 16, 67, 345)
Proportionate_sites <- c(20.45, 62.62, 168.70, 174.23)
k <- length(Actual_sites)
df <- k - 1
# Loop over the bin values to compute the chi2 test statistic
chi_square <- 0
for(i in 1:length(Actual_sites)){
chi_square <- chi_square + ((Actual_sites[i] - Proportionate_sites[i])^2 / Proportionate_sites[i])
}
chi_square
# looking up p value
p_value <- pchisq(chi_square, df=df, lower.tail=FALSE)
p_value
depression_yes <- 2607 / 50739
depression_yes
depression_no <- 1 - depression_yes
depression_no
p_value <- pchisq(20.93, df=df, lower.tail=FALSE)
p_value
ggplot(data=mlb11, aes(x=bat_avg, y=runs)) + geom_point()
load("more/mlb11.RData")
setwd("D:/CUNY/Courses/Statistics And Probability/CUNY-StatsAndProb/lab7")
load("more/mlb11.RData")
library(ggplot2)
ggplot(data=mlb11, aes(x=at_bats, y=runs)) + geom_point()
cor(mlb11$runs, mlb11$at_bats)
plot_ss(x = mlb11$at_bats, y = mlb11$runs)
m1 <- lm(runs ~ at_bats, data = mlb11)
summary(m1)
m2 <- lm(runs ~ homeruns, data = mlb11)
summary(m2)
plot(mlb11$runs ~ mlb11$at_bats)
abline(m1)
atbats <- 5578
runs_predicted = -2789.2429 + 0.6305 * atbats
runs_predicted
plot(m1$residuals ~ mlb11$at_bats)
abline(h = 0, lty = 3)  # adds a horizontal dashed line at y = 0
hist(m1$residuals)
qqnorm(m1$residuals)
qqline(m1$residuals)  # adds diagonal line to the normal prob plot
ggplot(data=mlb11, aes(x=bat_avg, y=runs)) + geom_point()
m3 <- lm(runs ~ bat_avg, data = mlb11)
summary(m3)
plot(mlb11$runs ~ mlb11$bat_avg)
abline(m3)
cor(mlb11$runs, mlb11$at_bats)
cor(mlb11$runs, mlb11$bat_avg)
cor(mlb11$runs, mlb11$at_bats)
cor(mlb11$runs, mlb11$hits)
cor(mlb11$runs, mlb11$homeruns)
cor(mlb11$runs, mlb11$bat_avg)
cor(mlb11$runs, mlb11$stolen_bases)
cor(mlb11$runs, mlb11$at_bats)
cor(mlb11$runs, mlb11$hits)
cor(mlb11$runs, mlb11$homeruns)
cor(mlb11$runs, mlb11$bat_avg)
cor(mlb11$runs, mlb11$stolen_bases)
cor(mlb11[,c(2,10,11,12)])
cor(mlb11$runs, mlb11$new_onbase)
cor(mlb11$runs, mlb11$new_obs)
cor(mlb11$runs, mlb11$new_obs)
cor(mlb11$runs, mlb11$new_onbase)
cor(mlb11$runs, mlb11$new_obs)
cor(mlb11$runs, mlb11$new_slug)
cor(mlb11$runs, mlb11$new_onbase)
cor(mlb11$runs, mlb11$new_obs)
cor(mlb11$runs, mlb11$new_slug)
ggplot(data=mlb11, aes(x=new_obs, y=runs)) + geom_point()
m4 <- lm(runs ~ new_obs, data = mlb11)
summary(m4)
